# LLM-Judge evaluation

TODOs:
* support m-arena-hard
* support dumping outputs
* test openai judge
* test vLLM judge
* handle errors
* CLI launcher
* document options
* CI
* cost?

Done:
* support alpaca-eval
* support arena-hard
* test together judge
* local env variable to set paths
* tqdm callback with batch
* support loading local completions

