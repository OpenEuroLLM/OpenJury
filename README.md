# LLM-Judge evaluation

TODOs:
* support loading local completions
* support m-arena-hard
* local env variable to set paths
* dump outputs
* test openai judge
* test vllm judge
* handle errors
* CLI launcher
* document options
* CI
* tqdm callback with batch
* cost?

Done:
* support alpaca-eval
* support arena-hard
* test together judge
